# ğŸ“š Real Time Speech to Text

A **real-time speech-to-text** system with **dual transcription modes**:
- **VOSK** â†’ Fully offline, low latency
- **Faster-Whisper** â†’ High accuracy with GPU/CPU support

The system:
- Streams audio from your microphone
- Transcribes in real time
- Saves transcripts to a text file
- Supports easy switching between modes in `config.py`

---

## ğŸ“Œ Features

- ğŸ¤ **Dual Mode** â€” Choose between:
  - **Offline Mode (VOSK)** â€” No internet needed, works on CPU
  - **High Accuracy Mode (Faster-Whisper)** â€” Uses GPU if available
- ğŸ“ **Transcript Saving** â€” Every recognized phrase is saved to `transcripts/transcript.txt`
- ğŸ–¥ **Real-time Display** â€” Shows partial and final transcriptions in the console
- âš¡ **Low Latency** â€” Processes audio in near real time
- ğŸ”„ **Easy Switching** â€” Change modes with one line in `config.py`

---

## ğŸ“‚ Folder Structure

realtime_speech_to_text/

â”‚

â”œâ”€â”€ main.py # Entry point to run the app

â”œâ”€â”€ config.py # Settings: mode, model paths, parameters

â”œâ”€â”€ vosk_streaming_stt.py # Offline low-latency transcription (VOSK)

â”œâ”€â”€ whisper_streaming_stt.py # High-accuracy GPU/CPU transcription (Faster-Whisper)

â””â”€â”€ transcripts/

â””â”€â”€ transcript.txt # Output file for transcripts


---

## âš™ï¸ Setup Guide

### **1ï¸âƒ£ Clone the repository**

```bash
git clone https://github.com/shameem3e/Real-Time-Speech-to-Text.git
cd Real-Time-Speech-to-Text

```
### **2ï¸âƒ£ Create Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scripts\activate      # Windows

```
### **3ï¸âƒ£ Install dependencies**
```bash
pip install --upgrade pip
pip install -r requirements.txt

```
Note:

* If using Whisper mode, you also need `ffmpeg` installed:

	* Windows: Install from [https://ffmpeg.org/download.html](https://ffmpeg.org/download.html) and add to PATH

	* macOS: `brew install ffmpeg`

	* Linux: `sudo apt install ffmpeg`

### **4ï¸âƒ£ Download models**
* VOSK (Offline mode):

	* Download from [https://alphacephei.com/vosk/models](https://alphacephei.com/vosk/models)

	* Example: `vosk-model-small-en-us-0.15`

	* Extract the folder and set the path in `config.py` â†’ `VOSK_MODEL_PATH`

* Faster-Whisper (High accuracy mode):

	* Model size is set in `config.py` â†’ `WHISPER_MODEL_SIZE`

	* Options: `"tiny"`, `"base"`, `"small"`, `"medium"`, `"large-v2"`

### **5ï¸âƒ£ Run the project**
```bash
python main.py

```
You should see: 
```bash
ğŸ¤ Real-Time Speech-to-Text with Transcript Saving
[INFO] Mode: VOSK
[INFO] Transcript will be saved to transcripts/transcript.txt

Partial: hello
You said: hello world

```

### **âœ… Example Output (VOSK Mode)**

You speak:
```bash
Hello, this is a test.

```

Console:
```bash
Partial: hello
You said: hello this is a test

```

transcripts/transcript.txt:
```bash
[2025-08-12 14:32:01] hello this is a test

```

## ğŸ“œ Code Overview

This project is divided into separate files for clarity and modularity:

### 1. `main.py`
- **Purpose:** Entry point of the project. Reads settings from `config.py` and launches the selected transcription mode.
- **Libraries used:**
  - `config` â†’ Load mode and settings.
  - `vosk_streaming_stt` / `whisper_streaming_stt` â†’ Calls the chosen engine.
- **Why:** Keeps the startup logic clean and mode switching simple.

---

### 2. `config.py`
- **Purpose:** Stores all configurable settings (mode, model paths, parameters).
- **Libraries used:** None (pure Python variables).
- **Why:** Allows changing between VOSK and Whisper without editing core code.

---

### 3. `vosk_streaming_stt.py`
- **Purpose:** Handles **offline low-latency transcription** using VOSK.
- **Libraries used:**
  - `vosk` â†’ Speech recognition model.
  - `sounddevice` â†’ Captures audio from microphone in real time.
  - `json` â†’ Parses recognition output from VOSK.
  - `datetime` â†’ Timestamping transcripts.
- **Why:** VOSK provides a fast and offline recognition option, ideal for low-resource environments.

---

### 4. `whisper_streaming_stt.py`
- **Purpose:** Handles **high-accuracy streaming transcription** using Faster-Whisper.
- **Libraries used:**
  - `faster_whisper` â†’ Loads and runs Whisper model with GPU/CPU.
  - `sounddevice` â†’ Captures audio chunks from microphone.
  - `soundfile` â†’ Temporarily saves chunks for processing.
  - `numpy` â†’ Handles audio buffers efficiently.
  - `datetime` â†’ Timestamping transcripts.
- **Why:** Faster-Whisper offers better accuracy for complex or accented speech, with GPU acceleration support.

---

### 5. `transcripts/transcript.txt`
- **Purpose:** Stores every recognized phrase with a timestamp.
- **Libraries used:** None (plain text file).
- **Why:** Keeps a permanent log for later review or analysis.

## â“ FAQ
Q: Do I need the internet?

A: No, if you use VOSK mode. Yes, if you use Whisper mode with models not cached locally.

Q: Does it work on GPU?

A: Yes, Faster-Whisper can use CUDA for acceleration.

Q: Can I use another language?

A: Yes, download the respective VOSK/Whisper language model.

Q: Where are transcripts saved?

A: In `transcripts/transcript.txt`


## ğŸ›  Tech Stack
* Python 3.8+
* VOSK
* Faster-Whisper
* Sounddevice
* Soundfile
* Numpy
* FFmpeg (for Whisper mode)

## ğŸš€ Future Improvements
* ğŸŒ Add Google Search integration for answering questions
* ğŸ“ Save transcripts in JSON format with timestamps
* ğŸ¯ Add speaker diarization (identify different speakers)
* ğŸ“¡ WebSocket API for real-time browser display
* ğŸ“± Mobile-friendly version

## ğŸ‘¨â€ğŸ’» Author
[MD. Shameem Ahammed](https://sites.google.com/view/shameem3e)

Graduate Student | AI & ML Enthusiast

---

---
